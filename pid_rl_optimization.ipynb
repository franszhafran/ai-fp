{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PID Parameter Optimization using Q-Learning\n",
    "## Optimizing Kp, Ki, Kd for TF.slx using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: MATLAB Engine Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "import os\n",
    "\n",
    "# Start MATLAB engine\n",
    "print(\"Starting MATLAB engine...\")\n",
    "eng = matlab.engine.start_matlab()\n",
    "print(\"MATLAB engine started successfully!\")\n",
    "\n",
    "# Set working directory\n",
    "work_dir = '/Users/franszhafran/code/master/ai/ai-final-project'\n",
    "eng.cd(work_dir, nargout=0)\n",
    "print(f\"Working directory set to: {work_dir}\")\n",
    "\n",
    "# Load EQ1.mat\n",
    "print(\"Loading EQ1.mat...\")\n",
    "eng.load('EQ1.mat', nargout=0)\n",
    "print(\"EQ1.mat loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Simulink Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulink(Kp, Ki, Kd, eng):\n",
    "    \"\"\"\n",
    "    Run Simulink model with given PID parameters.\n",
    "    \n",
    "    Args:\n",
    "        Kp (float): Proportional gain\n",
    "        Ki (float): Integral gain\n",
    "        Kd (float): Derivative gain\n",
    "        eng: MATLAB engine instance\n",
    "        \n",
    "    Returns:\n",
    "        float: RMSE value from simulation, or None if simulation failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set PID parameters in MATLAB workspace\n",
    "        eng.assignin('base', 'Kp', float(Kp), nargout=0)\n",
    "        eng.assignin('base', 'Ki', float(Ki), nargout=0)\n",
    "        eng.assignin('base', 'Kd', float(Kd), nargout=0)\n",
    "        \n",
    "        # Run simulation\n",
    "        eng.sim('TF', nargout=0)\n",
    "        \n",
    "        # Retrieve RMSE value\n",
    "        rmse_result = eng.evalin('base', 'rmse(end)')\n",
    "        rmse = float(rmse_result)\n",
    "        \n",
    "        return rmse\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Simulation error with Kp={Kp}, Ki={Ki}, Kd={Kd}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Simulink runner function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Q-Learning Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges and discretization\n",
    "KP_MIN, KP_MAX, KP_STEP = 0.1, 10.0, 0.5\n",
    "KI_MIN, KI_MAX, KI_STEP = 0.01, 5.0, 0.25\n",
    "KD_MIN, KD_MAX, KD_STEP = 0.01, 2.0, 0.1\n",
    "\n",
    "# Create parameter ranges\n",
    "kp_range = np.arange(KP_MIN, KP_MAX + KP_STEP/2, KP_STEP)\n",
    "ki_range = np.arange(KI_MIN, KI_MAX + KI_STEP/2, KI_STEP)\n",
    "kd_range = np.arange(KD_MIN, KD_MAX + KD_STEP/2, KD_STEP)\n",
    "\n",
    "print(f\"Kp range: {len(kp_range)} values from {KP_MIN} to {KP_MAX}\")\n",
    "print(f\"Ki range: {len(ki_range)} values from {KI_MIN} to {KI_MAX}\")\n",
    "print(f\"Kd range: {len(kd_range)} values from {KD_MIN} to {KD_MAX}\")\n",
    "print(f\"Total state space size: {len(kp_range) * len(ki_range) * len(kd_range)}\")\n",
    "\n",
    "# Define action space\n",
    "# Actions: 0=increase Kp, 1=decrease Kp, 2=increase Ki, 3=decrease Ki, 4=increase Kd, 5=decrease Kd\n",
    "NUM_ACTIONS = 6\n",
    "\n",
    "# Q-Learning hyperparameters\n",
    "ALPHA = 0.1  # Learning rate\n",
    "GAMMA = 0.95  # Discount factor\n",
    "EPSILON = 1.0  # Initial exploration rate\n",
    "EPSILON_DECAY = 0.995  # Exploration decay\n",
    "EPSILON_MIN = 0.01  # Minimum exploration rate\n",
    "NUM_EPISODES = 200  # Number of training episodes\n",
    "\n",
    "# Initialize Q-table as dictionary\n",
    "Q_table = defaultdict(lambda: np.zeros(NUM_ACTIONS))\n",
    "\n",
    "print(f\"\\nQ-Learning Configuration:\")\n",
    "print(f\"  Learning rate (alpha): {ALPHA}\")\n",
    "print(f\"  Discount factor (gamma): {GAMMA}\")\n",
    "print(f\"  Initial exploration rate: {EPSILON}\")\n",
    "print(f\"  Epsilon decay: {EPSILON_DECAY}\")\n",
    "print(f\"  Training episodes: {NUM_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "def action_to_string(action):\n    \"\"\"Convert action index to readable description.\"\"\"\n    action_map = {\n        0: \"â†‘ Increase Kp\",\n        1: \"â†“ Decrease Kp\",\n        2: \"â†‘ Increase Ki\",\n        3: \"â†“ Decrease Ki\",\n        4: \"â†‘ Increase Kd\",\n        5: \"â†“ Decrease Kd\"\n    }\n    return action_map.get(action, f\"Unknown action {action}\")\n\ndef calculate_q_stats(Q_table):\n    \"\"\"Calculate Q-value statistics across entire table.\"\"\"\n    all_q_values = []\n    for state in Q_table:\n        all_q_values.extend(Q_table[state])\n\n    if not all_q_values:\n        return {'avg': 0, 'max': 0, 'min': 0, 'std': 0}\n\n    all_q_values = np.array(all_q_values)\n    return {\n        'avg': np.mean(all_q_values),\n        'max': np.max(all_q_values),\n        'min': np.min(all_q_values),\n        'std': np.std(all_q_values)\n    }\n\nprint(\"Helper functions defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6: Helper Functions for Debugging",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Debugging and Logging Configuration\nVERBOSE_LEVEL = 1  # 0=minimal, 1=per-episode+events (DEFAULT), 2=per-step, 3=full debug\nLOG_Q_VALUES = True  # Track Q-value evolution (user priority)\nTRACK_CONVERGENCE = True  # Monitor parameter convergence (user priority)\nCONVERGENCE_WINDOW = 50   # Window size for convergence detection\nCONVERGENCE_THRESHOLD = 0.001  # RMSE variance threshold for convergence\n\nprint(\"Debugging configuration loaded:\")\nprint(f\"  VERBOSE_LEVEL: {VERBOSE_LEVEL}\")\nprint(f\"  LOG_Q_VALUES: {LOG_Q_VALUES}\")\nprint(f\"  TRACK_CONVERGENCE: {TRACK_CONVERGENCE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5: Debugging and Logging Configuration",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: State and Action Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_state(kp, ki, kd):\n",
    "    \"\"\"\n",
    "    Convert continuous PID parameters to state representation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (kp_idx, ki_idx, kd_idx) - indices in the discretized ranges\n",
    "    \"\"\"\n",
    "    kp_idx = int(np.round((kp - KP_MIN) / KP_STEP))\n",
    "    ki_idx = int(np.round((ki - KI_MIN) / KI_STEP))\n",
    "    kd_idx = int(np.round((kd - KD_MIN) / KD_STEP))\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    kp_idx = np.clip(kp_idx, 0, len(kp_range) - 1)\n",
    "    ki_idx = np.clip(ki_idx, 0, len(ki_range) - 1)\n",
    "    kd_idx = np.clip(kd_idx, 0, len(kd_range) - 1)\n",
    "    \n",
    "    return (kp_idx, ki_idx, kd_idx)\n",
    "\n",
    "def state_to_params(state):\n",
    "    \"\"\"\n",
    "    Convert state representation to continuous PID parameters.\n",
    "    \n",
    "    Args:\n",
    "        state: tuple (kp_idx, ki_idx, kd_idx)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Kp, Ki, Kd) continuous values\n",
    "    \"\"\"\n",
    "    kp_idx, ki_idx, kd_idx = state\n",
    "    kp = kp_range[kp_idx]\n",
    "    ki = ki_range[ki_idx]\n",
    "    kd = kd_range[kd_idx]\n",
    "    return (kp, ki, kd)\n",
    "\n",
    "def take_action(state, action):\n",
    "    \"\"\"\n",
    "    Take an action in the state space.\n",
    "    \n",
    "    Args:\n",
    "        state: tuple (kp_idx, ki_idx, kd_idx)\n",
    "        action: int from 0 to 5\n",
    "        \n",
    "    Returns:\n",
    "        tuple: new_state\n",
    "    \"\"\"\n",
    "    kp_idx, ki_idx, kd_idx = state\n",
    "    \n",
    "    if action == 0:  # Increase Kp\n",
    "        kp_idx = min(kp_idx + 1, len(kp_range) - 1)\n",
    "    elif action == 1:  # Decrease Kp\n",
    "        kp_idx = max(kp_idx - 1, 0)\n",
    "    elif action == 2:  # Increase Ki\n",
    "        ki_idx = min(ki_idx + 1, len(ki_range) - 1)\n",
    "    elif action == 3:  # Decrease Ki\n",
    "        ki_idx = max(ki_idx - 1, 0)\n",
    "    elif action == 4:  # Increase Kd\n",
    "        kd_idx = min(kd_idx + 1, len(kd_range) - 1)\n",
    "    elif action == 5:  # Decrease Kd\n",
    "        kd_idx = max(kd_idx - 1, 0)\n",
    "    \n",
    "    return (kp_idx, ki_idx, kd_idx)\n",
    "\n",
    "print(\"State and action functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(rmse):\n",
    "    \"\"\"\n",
    "    Convert RMSE to reward signal.\n",
    "    Lower RMSE = higher reward (less error = more positive reward).\n",
    "    \n",
    "    Args:\n",
    "        rmse (float): Root Mean Square Error from simulation\n",
    "        \n",
    "    Returns:\n",
    "        float: Reward value\n",
    "    \"\"\"\n",
    "    if rmse is None:\n",
    "        return -100  # Penalty for simulation failure\n",
    "    return -rmse  # Negative RMSE as reward (minimizing RMSE)\n",
    "\n",
    "print(\"Reward function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Q-Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize training variables\nepsilon = EPSILON\ntraining_history = {\n    'episode': [],\n    'rmse': [],\n    'best_rmse': [],\n    'kp': [],\n    'ki': [],\n    'kd': [],\n    'epsilon': []\n}\n\n# Q-Learning tracking (user priority: Q-learning internals)\nq_value_history = {\n    'episode': [],\n    'avg_q_value': [],\n    'max_q_value': [],\n    'min_q_value': [],\n    'q_value_std': []\n}\n\n# Convergence tracking (user priority: parameter convergence)\nconvergence_status = {\n    'is_converged': False,\n    'convergence_episode': None,\n    'steps_without_improvement': 0\n}\n\nsimulation_stats = {\n    'total_sims': 0,\n    'successful_sims': 0,\n    'failed_sims': 0\n}\n\nbest_rmse = float('inf')\nbest_params = None\n\n# Initial state: use middle values of parameter ranges\ninitial_kp_idx = len(kp_range) // 2\ninitial_ki_idx = len(ki_range) // 2\ninitial_kd_idx = len(kd_range) // 2\ninitial_state = (initial_kp_idx, initial_ki_idx, initial_kd_idx)\n\nprint(f\"Starting Q-Learning Training...\")\nprint(f\"Initial state parameters: {state_to_params(initial_state)}\")\nprint(\"\\n\" + \"=\"*70)\n\nstart_time = time.time()\n\nfor episode in range(NUM_EPISODES):\n    state = initial_state\n    episode_rmse = []\n    episode_td_errors = []\n    episode_q_updates = []\n    \n    # Episode loop\n    for step in range(10):  # 10 steps per episode\n        # Epsilon-greedy action selection\n        is_explore = np.random.random() < epsilon\n        if is_explore:\n            # Explore: random action\n            action = np.random.randint(0, NUM_ACTIONS)\n        else:\n            # Exploit: best known action\n            action = np.argmax(Q_table[state])\n        \n        # Take action and get new state\n        new_state = take_action(state, action)\n        \n        # Run Simulink and get reward\n        kp, ki, kd = state_to_params(new_state)\n        rmse = run_simulink(kp, ki, kd, eng)\n        reward = calculate_reward(rmse)\n        \n        # Track simulation statistics\n        simulation_stats['total_sims'] += 1\n        if rmse is not None:\n            simulation_stats['successful_sims'] += 1\n        else:\n            simulation_stats['failed_sims'] += 1\n            if VERBOSE_LEVEL >= 1:\n                print(f\"  âš  Simulation failed: Kp={kp:.4f}, Ki={ki:.4f}, Kd={kd:.4f}\")\n        \n        if rmse is not None:\n            episode_rmse.append(rmse)\n            \n            # Enhanced logging when best parameters update (KEY EVENT)\n            if rmse < best_rmse:\n                old_best = best_rmse\n                best_rmse = rmse\n                best_params = (kp, ki, kd)\n                convergence_status['steps_without_improvement'] = 0\n\n                if VERBOSE_LEVEL >= 1:\n                    improvement = ((old_best - best_rmse) / old_best) * 100\n                    print(f\"  âœ“ NEW BEST! RMSE: {old_best:.6f} â†’ {best_rmse:.6f} ({improvement:.2f}% better)\")\n                    print(f\"    Parameters: Kp={kp:.4f}, Ki={ki:.4f}, Kd={kd:.4f}\")\n            else:\n                convergence_status['steps_without_improvement'] += 1\n        \n        # Store Q-value before update for tracking (user priority)\n        if LOG_Q_VALUES:\n            q_before = Q_table[state][action]\n        \n        # Q-Learning update\n        max_next_q = np.max(Q_table[new_state])\n        Q_table[state][action] = Q_table[state][action] + ALPHA * (reward + GAMMA * max_next_q - Q_table[state][action])\n        \n        # Track Q-value change (user priority: Q-learning internals)\n        if LOG_Q_VALUES:\n            q_after = Q_table[state][action]\n            td_error = reward + GAMMA * max_next_q - q_before\n            episode_td_errors.append(abs(td_error))\n            episode_q_updates.append(abs(q_after - q_before))\n        \n        state = new_state\n    \n    # Decay exploration rate\n    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n    \n    # Calculate Q-value statistics for this episode (user priority)\n    if LOG_Q_VALUES:\n        all_q_values = []\n        for s in Q_table:\n            all_q_values.extend(Q_table[s])\n\n        q_value_history['episode'].append(episode + 1)\n        q_value_history['avg_q_value'].append(np.mean(all_q_values))\n        q_value_history['max_q_value'].append(np.max(all_q_values))\n        q_value_history['min_q_value'].append(np.min(all_q_values))\n        q_value_history['q_value_std'].append(np.std(all_q_values))\n\n        avg_td_error = np.mean(episode_td_errors) if episode_td_errors else 0\n        avg_q_update = np.mean(episode_q_updates) if episode_q_updates else 0\n    \n    # Record episode statistics\n    avg_rmse = np.mean(episode_rmse) if episode_rmse else None\n    if avg_rmse is not None:\n        training_history['episode'].append(episode + 1)\n        training_history['rmse'].append(avg_rmse)\n        training_history['best_rmse'].append(best_rmse)\n        training_history['epsilon'].append(epsilon)\n        \n        if best_params:\n            training_history['kp'].append(best_params[0])\n            training_history['ki'].append(best_params[1])\n            training_history['kd'].append(best_params[2])\n    \n    # Enhanced per-episode logging (VERBOSE_LEVEL >= 1, default)\n    if VERBOSE_LEVEL >= 1 and avg_rmse is not None:\n        success_rate = (simulation_stats['successful_sims'] /\n                       simulation_stats['total_sims'] * 100)\n\n        print(f\"Ep {episode + 1:3d}/{NUM_EPISODES} | \"\n              f\"RMSE: {avg_rmse:.6f} | \"\n              f\"Best: {best_rmse:.6f} | \"\n              f\"Îµ: {epsilon:.4f} | \"\n              f\"Success: {success_rate:.1f}%\", end=\"\")\n\n        if LOG_Q_VALUES:\n            print(f\" | Q-avg: {q_value_history['avg_q_value'][-1]:.4f} | \"\n                  f\"TD-err: {avg_td_error:.4f}\")\n        else:\n            print()\n    \n    # Check convergence (user priority: parameter convergence)\n    if TRACK_CONVERGENCE and len(training_history['rmse']) >= CONVERGENCE_WINDOW:\n        recent_rmse = training_history['rmse'][-CONVERGENCE_WINDOW:]\n        rmse_variance = np.var(recent_rmse)\n\n        if rmse_variance < CONVERGENCE_THRESHOLD and not convergence_status['is_converged']:\n            convergence_status['is_converged'] = True\n            convergence_status['convergence_episode'] = episode + 1\n\n            if VERBOSE_LEVEL >= 1:\n                print(f\"  ðŸŽ¯ CONVERGENCE DETECTED at Episode {episode + 1}\")\n                print(f\"    RMSE variance over last {CONVERGENCE_WINDOW} episodes: {rmse_variance:.6f}\")\n                print(f\"    Steps without improvement: {convergence_status['steps_without_improvement']}\")\n\nelapsed_total = time.time() - start_time\nprint(\"=\"*70)\nprint(f\"\\nTraining completed in {elapsed_total:.1f} seconds\")\nprint(f\"\\nBest Parameters Found:\")\nprint(f\"  Kp = {best_params[0]:.4f}\")\nprint(f\"  Ki = {best_params[1]:.4f}\")\nprint(f\"  Kd = {best_params[2]:.4f}\")\nprint(f\"  RMSE = {best_rmse:.6f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Visualize Q-value evolution (user priority)\nif LOG_Q_VALUES and len(q_value_history['episode']) > 0:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Plot 1: Q-value evolution over episodes\n    ax1.plot(q_value_history['episode'], q_value_history['avg_q_value'],\n             label='Avg Q-value', linewidth=2, color='blue')\n    ax1.fill_between(q_value_history['episode'],\n                     q_value_history['min_q_value'],\n                     q_value_history['max_q_value'],\n                     alpha=0.2, color='blue', label='Min-Max Range')\n    ax1.set_xlabel('Episode')\n    ax1.set_ylabel('Q-Value')\n    ax1.set_title('Q-Value Evolution During Training')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Plot 2: RMSE vs Q-value correlation\n    ax2.scatter(q_value_history['avg_q_value'][:len(training_history['best_rmse'])], \n                training_history['best_rmse'][:len(q_value_history['avg_q_value'])],\n                alpha=0.6, s=30, color='red')\n    ax2.set_xlabel('Average Q-Value')\n    ax2.set_ylabel('Best RMSE')\n    ax2.set_title('Q-Value vs Performance Correlation')\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig('q_value_analysis.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n    print(\"\\nQ-value analysis saved as 'q_value_analysis.png'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 7b: Q-Value Visualization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Print focused training statistics\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING SUMMARY - CONVERGENCE & Q-LEARNING ANALYSIS\")\nprint(\"=\"*70)\n\n# Convergence Analysis\nprint(f\"\\nConvergence Status:\")\nif convergence_status['is_converged']:\n    print(f\"  âœ“ Converged at episode {convergence_status['convergence_episode']}\")\n    print(f\"  Remaining episodes after convergence: \"\n          f\"{NUM_EPISODES - convergence_status['convergence_episode']}\")\nelse:\n    print(f\"  âš  Did not converge within {NUM_EPISODES} episodes\")\nprint(f\"  Final steps without improvement: {convergence_status['steps_without_improvement']}\")\n\n# Q-Learning Internals\nif LOG_Q_VALUES and len(q_value_history['episode']) > 0:\n    print(f\"\\nQ-Value Evolution:\")\n    print(f\"  Initial avg Q-value: {q_value_history['avg_q_value'][0]:.6f}\")\n    print(f\"  Final avg Q-value: {q_value_history['avg_q_value'][-1]:.6f}\")\n    print(f\"  Q-value range: [{q_value_history['min_q_value'][-1]:.6f}, \"\n          f\"{q_value_history['max_q_value'][-1]:.6f}]\")\n    print(f\"  Final Q-value std dev: {q_value_history['q_value_std'][-1]:.6f}\")\n\n    # Detect if Q-values are converging\n    if len(q_value_history['avg_q_value']) >= 20:\n        recent_change = abs(q_value_history['avg_q_value'][-1] -\n                          q_value_history['avg_q_value'][-20])\n        print(f\"  Q-value change (last 20 episodes): {recent_change:.6f}\")\n\nprint(f\"\\nState Space Exploration:\")\nprint(f\"  Unique states visited: {len(Q_table)}\")\ntotal_states = len(kp_range) * len(ki_range) * len(kd_range)\nprint(f\"  State space coverage: {len(Q_table)/total_states*100:.2f}% \"\n      f\"({len(Q_table)}/{total_states})\")\n\nprint(f\"\\nSimulation Statistics:\")\nsuccess_rate = (simulation_stats['successful_sims']/simulation_stats['total_sims']*100\n                if simulation_stats['total_sims'] > 0 else 0)\nprint(f\"  Total simulations: {simulation_stats['total_sims']}\")\nprint(f\"  Success rate: {success_rate:.2f}% \"\n      f\"({simulation_stats['successful_sims']}/{simulation_stats['total_sims']})\")\nif simulation_stats['failed_sims'] > 0:\n    print(f\"  âš  Failed simulations: {simulation_stats['failed_sims']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 7a: Post-Training Debug Summary",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: RMSE over episodes\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(training_history['episode'], training_history['rmse'], label='Episode Avg RMSE', alpha=0.7)\n",
    "ax1.plot(training_history['episode'], training_history['best_rmse'], label='Best RMSE', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Training Progress: RMSE over Episodes')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter evolution - Kp\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(training_history['episode'], training_history['kp'], color='red', label='Kp', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Kp Value')\n",
    "ax2.set_title('Proportional Gain (Kp) Evolution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=best_params[0], color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Parameter evolution - Ki\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(training_history['episode'], training_history['ki'], color='green', label='Ki', linewidth=2)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Ki Value')\n",
    "ax3.set_title('Integral Gain (Ki) Evolution')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=best_params[1], color='g', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 4: Parameter evolution - Kd\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(training_history['episode'], training_history['kd'], color='blue', label='Kd', linewidth=2)\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Kd Value')\n",
    "ax4.set_title('Derivative Gain (Kd) Evolution')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(y=best_params[2], color='b', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pid_training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining visualization saved as 'pid_training_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Q-Table Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-table\n",
    "q_values = []\n",
    "for state in Q_table:\n",
    "    q_values.extend(Q_table[state])\n",
    "\n",
    "q_values = np.array(q_values)\n",
    "\n",
    "print(\"Q-Table Statistics:\")\n",
    "print(f\"  Total states explored: {len(Q_table)}\")\n",
    "print(f\"  Total Q-values: {len(q_values)}\")\n",
    "print(f\"  Mean Q-value: {np.mean(q_values):.6f}\")\n",
    "print(f\"  Min Q-value: {np.min(q_values):.6f}\")\n",
    "print(f\"  Max Q-value: {np.max(q_values):.6f}\")\n",
    "print(f\"  Std Q-value: {np.std(q_values):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Validation Run with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValidation Run with Best Parameters Found\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run simulation with best parameters\n",
    "final_rmse = run_simulink(best_params[0], best_params[1], best_params[2], eng)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Kp = {best_params[0]:.6f}\")\n",
    "print(f\"  Ki = {best_params[1]:.6f}\")\n",
    "print(f\"  Kd = {best_params[2]:.6f}\")\n",
    "print(f\"  RMSE = {final_rmse:.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with initial parameters\n",
    "initial_kp, initial_ki, initial_kd = state_to_params(initial_state)\n",
    "initial_rmse = run_simulink(initial_kp, initial_ki, initial_kd, eng)\n",
    "\n",
    "print(f\"\\nComparison with Initial Parameters:\")\n",
    "print(f\"  Initial: Kp={initial_kp:.6f}, Ki={initial_ki:.6f}, Kd={initial_kd:.6f}, RMSE={initial_rmse:.6f}\")\n",
    "print(f\"  Optimized: Kp={best_params[0]:.6f}, Ki={best_params[1]:.6f}, Kd={best_params[2]:.6f}, RMSE={final_rmse:.6f}\")\n",
    "improvement = ((initial_rmse - final_rmse) / initial_rmse) * 100\n",
    "print(f\"  Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'best_parameters': {\n",
    "        'Kp': float(best_params[0]),\n",
    "        'Ki': float(best_params[1]),\n",
    "        'Kd': float(best_params[2]),\n",
    "    },\n",
    "    'best_rmse': float(best_rmse),\n",
    "    'training_episodes': NUM_EPISODES,\n",
    "    'training_time_seconds': elapsed_total,\n",
    "    'improvement_percent': improvement\n",
    "}\n",
    "\n",
    "with open('pid_optimization_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResults exported to 'pid_optimization_results.json'\")\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close MATLAB engine\n",
    "print(\"Closing MATLAB engine...\")\n",
    "eng.quit()\n",
    "print(\"MATLAB engine closed successfully!\")\n",
    "print(\"\\nOptimization workflow completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}